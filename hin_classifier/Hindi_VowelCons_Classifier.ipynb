{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PyTorch Data Loader Code for Vowel Consonant Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PeozigfFQ850",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "#For converting the dataset to torchvision dataset format\n",
        "class VowelConsonantDataset(Dataset):\n",
        "    def __init__(self, file_path,train=True,transform=None):\n",
        "        self.transform = transform\n",
        "        self.file_path=file_path\n",
        "        self.train=train\n",
        "        self.file_names=[file for _,_,files in os.walk(self.file_path) for file in files]\n",
        "        self.len = len(self.file_names)\n",
        "        if self.train:\n",
        "            self.classes_mapping=self.get_classes()\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        file_name=self.file_names[index]\n",
        "        image_data=self.pil_loader(self.file_path+\"/\"+file_name)\n",
        "        if self.transform:\n",
        "            image_data = self.transform(image_data)\n",
        "        if self.train:\n",
        "            file_name_splitted=file_name.split(\"_\")\n",
        "            Y1 = self.classes_mapping[file_name_splitted[0]]\n",
        "            Y2 = self.classes_mapping[file_name_splitted[1]]\n",
        "            z1,z2=torch.zeros(10),torch.zeros(10)\n",
        "            z1[Y1-10],z2[Y2]=1,1\n",
        "            label=torch.stack([z1,z2])\n",
        "\n",
        "            return image_data, label\n",
        "\n",
        "        else:\n",
        "            vow_test_tensor, con_test_tensor = torch.zeros(10,dtype=torch.int64), torch.zeros(10,dtype=torch.int64)\n",
        "            numeric = file_name.split('.')[0]\n",
        "            if len(numeric) < 4:\n",
        "              numeric = '0'*(4-len(numeric))+numeric\n",
        "            if numeric == '10000':\n",
        "              numeric = '9999'\n",
        "            vow_test_tensor[int(numeric[0])] = 1\n",
        "            con_test_tensor[int(numeric[1])] = 1\n",
        "            test_label = torch.stack([vow_test_tensor,con_test_tensor])\n",
        "            return image_data, test_label\n",
        "          \n",
        "    def pil_loader(self,path):\n",
        "        with open(path, 'rb') as f:\n",
        "            img = Image.open(f)\n",
        "            return img.convert('RGB')\n",
        "\n",
        "      \n",
        "    def get_classes(self):\n",
        "        classes=[]\n",
        "        for name in self.file_names:\n",
        "            name_splitted=name.split(\"_\")\n",
        "            classes.extend([name_splitted[0],name_splitted[1]])\n",
        "        classes=list(set(classes))\n",
        "        classes_mapping={}\n",
        "        for i,cl in enumerate(sorted(classes)):\n",
        "            classes_mapping[cl]=i\n",
        "        return classes_mapping\n",
        "    "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rJJ_MrEVQ_eX",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1Gg9mEGh3W2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0d197e07-c806-4d12-c391-ee0ca6c52a71"
      },
      "source": [
        "import importlib\n",
        "if importlib.util.find_spec('mlflow') is None:\n",
        "  !pip install mlflow\n",
        "import mlflow\n",
        "import mlflow.pytorch"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mlflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/bb/c79f745214c39dd70b8596b8341c4a6f93ec96f6ed7c7a769c6a826d215f/mlflow-1.9.1-py3-none-any.whl (11.9MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0MB 253kB/s \n",
            "\u001b[?25hCollecting azure-storage-blob>=12.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/3d/31614573e8a197db12d8ab47a7fd813f15bd4a4b5c64e85d23b865de5b9b/azure_storage_blob-12.3.2-py2.py3-none-any.whl (280kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 40.6MB/s \n",
            "\u001b[?25hCollecting querystring-parser\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/fa/f54f5662e0eababf0c49e92fd94bf178888562c0e7b677c8941bbbcd1bd6/querystring_parser-1.2.4.tar.gz\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (7.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from mlflow) (3.13)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.18.5)\n",
            "Collecting gorilla\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/56/5a683944cbfc77e429c6f03c636ca50504a785f60ffae91ddd7f5f7bb520/gorilla-0.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.0.5)\n",
            "Requirement already satisfied: sqlparse in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.3.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.3.0)\n",
            "Collecting databricks-cli>=0.8.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/57/5c2d6b83cb8753d12f548e89f91037632baa8289677c1b2ab2adf14bf6b2/databricks-cli-0.11.0.tar.gz (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.0MB/s \n",
            "\u001b[?25hCollecting gitpython>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/f9/c315aa88e51fabdc08e91b333cfefb255aff04a2ee96d632c32cb19180c9/GitPython-3.1.3-py3-none-any.whl (451kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 34.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.1.2)\n",
            "Collecting prometheus-flask-exporter\n",
            "  Downloading https://files.pythonhosted.org/packages/24/c4/c60d676f33f12f7c64c684bd6a6b82d4605c12b6ff1ca412b8a0449cf08a/prometheus_flask_exporter-0.14.1.tar.gz\n",
            "Collecting docker>=4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/15/7a2f095a3b8b0fff9a0a5f56bd941e05fa958d4ca31105541001a5f7d3eb/docker-4.2.2-py2.py3-none-any.whl (144kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 34.2MB/s \n",
            "\u001b[?25hCollecting sqlalchemy<=1.3.13\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/47/35edeb0f86c0b44934c05d961c893e223ef27e79e1f53b5e6f14820ff553/SQLAlchemy-1.3.13.tar.gz (6.0MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0MB 34.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from mlflow) (2.8.1)\n",
            "Collecting gunicorn; platform_system != \"Windows\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/ca/926f7cd3a2014b16870086b2d0fdc84a9e49473c68a8dff8b57f7c156f43/gunicorn-20.0.4-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 508kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.17.3 in /usr/local/lib/python3.6/dist-packages (from mlflow) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.12.0)\n",
            "Collecting alembic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/1e/cabc75a189de0fbb2841d0975243e59bde8b7822bacbb95008ac6fe9ad47/alembic-1.4.2.tar.gz (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 33.4MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting azure-core<2.0.0,>=1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/ed/ff9f4669e5b9a78a62fe43b83cc01e9007bbf6e99ec7ab6f73557135099f/azure_core-1.6.0-py2.py3-none-any.whl (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 34.3MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.1.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/04/686efee2dcdd25aecf357992e7d9362f443eb182ecd623f882bc9f7a6bba/cryptography-2.9.2-cp35-abi3-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 37.7MB/s \n",
            "\u001b[?25hCollecting msrest>=0.6.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/b4/a95380d5199c4785b318038db6d199a203f6970188876b473c00533bc17f/msrest-0.6.17-py2.py3-none-any.whl (84kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->mlflow) (2018.9)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from databricks-cli>=0.8.7->mlflow) (0.8.7)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.0->mlflow) (47.3.1)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow) (2.11.2)\n",
            "Requirement already satisfied: prometheus_client in /usr/local/lib/python3.6/dist-packages (from prometheus-flask-exporter->mlflow) (0.8.0)\n",
            "Collecting websocket-client>=0.32.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 34.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (2.9)\n",
            "Collecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.7MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.1.4->azure-storage-blob>=12.0->mlflow) (1.14.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from msrest>=0.6.10->azure-storage-blob>=12.0->mlflow) (1.3.0)\n",
            "Collecting isodate>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.5MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask->mlflow) (1.1.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.1.4->azure-storage-blob>=12.0->mlflow) (2.20)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.10->azure-storage-blob>=12.0->mlflow) (3.1.0)\n",
            "Building wheels for collected packages: alembic\n",
            "  Building wheel for alembic (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.4.2-cp36-none-any.whl size=159543 sha256=42e83291498e6acc408dd488a859d00803f0449eacee8c0aeec20cc4e2332b80\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/04/83/76023f7a4c14688c0b5c2682a96392cfdd3ee4449eaaa287ef\n",
            "Successfully built alembic\n",
            "Building wheels for collected packages: querystring-parser, databricks-cli, prometheus-flask-exporter, sqlalchemy\n",
            "  Building wheel for querystring-parser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for querystring-parser: filename=querystring_parser-1.2.4-cp36-none-any.whl size=7079 sha256=2db1757e0276790a8bd7655cbba424011cda3dd2cb0897f450820cc1b4a1b09e\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/41/34/23ebf5d1089a9aed847951e0ee375426eb4ad0a7079d88d41e\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.11.0-cp36-none-any.whl size=90300 sha256=dda17b1eda1e8d45bb2ce90a51674ed5d0c81e83b3340c53b74dbac9fc263152\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/d0/4f/3deeca1f4c47a6aca7c2c6a6e2bf272391565dc86a7718a59b\n",
            "  Building wheel for prometheus-flask-exporter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.14.1-cp36-none-any.whl size=15200 sha256=2eed3a28ebed8b75ae00ff07bd50f5369a6a6ce35cd38129a869115fbc7ad4e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/36/0f/7b80b9e6cf5fcd81034fe9b5470937f3a2861ec2d47997ffd2\n",
            "  Building wheel for sqlalchemy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlalchemy: filename=SQLAlchemy-1.3.13-cp36-cp36m-linux_x86_64.whl size=1217133 sha256=ba4309d8133ad690a892eb20cd4622769c2e0f4896f6d13b39286ca4bbad3706\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/35/98/4c9cb3fd63d21d5606b972dd70643769745adf60e622467b71\n",
            "Successfully built querystring-parser databricks-cli prometheus-flask-exporter sqlalchemy\n",
            "Installing collected packages: azure-core, cryptography, isodate, msrest, azure-storage-blob, querystring-parser, gorilla, databricks-cli, smmap, gitdb, gitpython, prometheus-flask-exporter, websocket-client, docker, sqlalchemy, gunicorn, Mako, python-editor, alembic, mlflow\n",
            "  Found existing installation: SQLAlchemy 1.3.17\n",
            "    Uninstalling SQLAlchemy-1.3.17:\n",
            "      Successfully uninstalled SQLAlchemy-1.3.17\n",
            "Successfully installed Mako-1.1.3 alembic-1.4.2 azure-core-1.6.0 azure-storage-blob-12.3.2 cryptography-2.9.2 databricks-cli-0.11.0 docker-4.2.2 gitdb-4.0.5 gitpython-3.1.3 gorilla-0.3.0 gunicorn-20.0.4 isodate-0.6.0 mlflow-1.9.1 msrest-0.6.17 prometheus-flask-exporter-0.14.1 python-editor-1.0.4 querystring-parser-1.2.4 smmap-3.0.4 sqlalchemy-1.3.13 websocket-client-0.57.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehfx2lqDBvVL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "21c8e2ce-7f6f-4bae-fab9-21cc89641c1a"
      },
      "source": [
        "if importlib.util.find_spec('google.colab'):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRx5cA3oByw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p hin_classifier\n",
        "!unzip -nq \"/content/drive/My Drive/hin_classifier/train.zip\" -d hin_classifier\n",
        "!unzip -nq \"/content/drive/My Drive/hin_classifier/test.zip\" -d hin_classifier"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmIYf267CIy6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "60db9974-cef6-40c0-e695-173cdf60be0c"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S-BX7SIgS_bU",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLujQRYBjUI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Params(object):\n",
        "    def __init__(self, batch_size, epochs, seed, log_interval):\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.seed = seed\n",
        "        self.log_interval = log_interval\n",
        "\n",
        "max_epochs = 16 if torch.cuda.is_available() else 2\n",
        "args = Params(256, max_epochs, 0, 20)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X2cOje6hS_yA",
        "colab": {}
      },
      "source": [
        "full_data = VowelConsonantDataset(\"hin_classifier/train\",train=True,transform=transform)\n",
        "train_size = int(0.9 * len(full_data))\n",
        "test_size = len(full_data) - train_size\n",
        "\n",
        "train_data, validation_data = random_split(full_data, [train_size, test_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_data, batch_size=args.batch_size, shuffle=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Uzv8q9j63A_L",
        "colab": {}
      },
      "source": [
        "test_data = VowelConsonantDataset(\"hin_classifier/test\",train=False,transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=args.batch_size,shuffle=False)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZtMz5RkiROn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def label_tensor(actual_labels):\n",
        "    return torch.LongTensor([torch.max(labels, dim = -1)[1].item() for labels in actual_labels])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsxxftMkSYxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "    def __init__(self): \n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(12288, 48),  # 64 x 64 x 3 = 12288\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(48, 24),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(24, 10)\n",
        "        )\n",
        "             \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-29aucGx3Tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cumulative_loss(predicted, actual, compute_loss=nn.CrossEntropyLoss()):\n",
        "  actual_reshaped = actual.permute(1,0,2)\n",
        "  vow_labels = label_tensor(actual_reshaped[0])\n",
        "  con_labels = label_tensor(actual_reshaped[1])\n",
        "  vow_labels = vow_labels.to(device)\n",
        "  con_labels = con_labels.to(device)\n",
        "\n",
        "  loss = compute_loss(predicted, vow_labels)\n",
        "  loss += compute_loss(predicted, con_labels)\n",
        "  return loss\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sobLv8Q-gVeo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(network, optimise, epoch, compute_loss):\n",
        "    \n",
        "  network.train()\n",
        "  for batch_id, data in enumerate(train_loader):\n",
        "    inputs, labels = data\n",
        "            \n",
        "    optimise.zero_grad()\n",
        "    inputs = inputs.to(device)\n",
        "    outputs = network(inputs)\n",
        "                  \n",
        "    loss = cumulative_loss(outputs, labels)\n",
        "\n",
        "    loss.backward()\n",
        "    optimise.step()\n",
        "      \n",
        "    if batch_id % args.log_interval == 0:\n",
        "      pos = epoch * len(train_loader) + batch_id\n",
        "      mlflow.log_metric('train_loss', loss.data.item()/len(inputs)*1000)\n",
        "          \n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.3f}'.format(\n",
        "              epoch, batch_id * len(inputs), len(train_loader.dataset),\n",
        "              100. * batch_id / len(train_loader), loss.data.item()))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub_3LZJJ834B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(network, epoch, compute_loss):\n",
        "    \n",
        "    network.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    confusion_matrix = np.zeros([10, 10])\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = network(inputs)\n",
        "            test_loss += compute_loss(outputs, labels).data.item()\n",
        "            pred = outputs.data.max(1)[1]\n",
        "            labels_reshaped = labels.permute(1,0,2)\n",
        "            vow_test = label_tensor(labels_reshaped[0].data).to(device)\n",
        "            con_test = label_tensor(labels_reshaped[1].data).to(device)\n",
        "            correct += pred.eq(vow_test).sum().item()\n",
        "            correct += pred.eq(con_test).sum().item()\n",
        "\n",
        "            \n",
        "            for x, y in zip(pred.cpu().numpy(), labels.numpy()):\n",
        "                confusion_matrix[x][y] += 1\n",
        "            \n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        test_accuracy = 100.0 * correct / len(test_loader.dataset)\n",
        "        \n",
        "        pos = (epoch + 1) * len(train_loader)\n",
        "        mlflow.log_metric('test_loss', test_loss*1000)\n",
        "        mlflow.log_metric('test_accuracy', test_accuracy)\n",
        "        \n",
        "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.\n",
        "              format(test_loss, correct, len(test_loader.dataset), test_accuracy))\n",
        "              \n",
        "        if epoch == args.epochs:\n",
        "            classes = np.arange(10)\n",
        "            fig, ax = plt.subplots()\n",
        "            im = ax.imshow(confusion_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "            ax.figure.colorbar(im, ax=ax)\n",
        "            ax.set(xticks=np.arange(confusion_matrix.shape[1]),\n",
        "                       yticks=np.arange(confusion_matrix.shape[0]),\n",
        "                       xticklabels=classes, yticklabels=classes,\n",
        "                       ylabel='True label',\n",
        "                       xlabel='Predicted label',\n",
        "                       title='Epoch %d' % epoch)\n",
        "            thresh = confusion_matrix.max() / 2.\n",
        "            for i in range(confusion_matrix.shape[0]):\n",
        "                for j in range(confusion_matrix.shape[1]):\n",
        "                    ax.text(j, i, int(confusion_matrix[i, j]),\n",
        "                            ha=\"center\", va=\"center\",\n",
        "                            color=\"white\" if confusion_matrix[i, j] > thresh else \"black\")\n",
        "              \n",
        "            fig.tight_layout()\n",
        "              \n",
        "            image_path = 'images/%s.png' % (expt_id)\n",
        "            plt.savefig(image_path)\n",
        "            mlflow.log_artifact(image_path)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E596-_UES2m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7384af6d-ddc2-45ea-d56d-4b9ac099ee9c"
      },
      "source": [
        "%%time\n",
        "for epoch in range(args.epochs):\n",
        "  ffn = FeedForwardNetwork().to(device)\n",
        "  train(ffn, optim.Adam(ffn.parameters()), epoch,cumulative_loss)\n",
        "  test(ffn, epoch,cumulative_loss)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/9000 (0%)]\tLoss: 4.611\n",
            "Train Epoch: 0 [5120/9000 (56%)]\tLoss: 4.621\n",
            "\n",
            "Test set: Average loss: 0.0185, Accuracy: 1995/10000 (20%)\n",
            "\n",
            "Train Epoch: 1 [0/9000 (0%)]\tLoss: 4.614\n",
            "Train Epoch: 1 [5120/9000 (56%)]\tLoss: 4.622\n",
            "\n",
            "Test set: Average loss: 0.0185, Accuracy: 2000/10000 (20%)\n",
            "\n",
            "Train Epoch: 2 [0/9000 (0%)]\tLoss: 4.612\n",
            "Train Epoch: 2 [5120/9000 (56%)]\tLoss: 4.598\n",
            "\n",
            "Test set: Average loss: 0.0185, Accuracy: 2022/10000 (20%)\n",
            "\n",
            "Train Epoch: 3 [0/9000 (0%)]\tLoss: 4.601\n",
            "Train Epoch: 3 [5120/9000 (56%)]\tLoss: 4.645\n",
            "\n",
            "Test set: Average loss: 0.0185, Accuracy: 2071/10000 (21%)\n",
            "\n",
            "Train Epoch: 4 [0/9000 (0%)]\tLoss: 4.630\n",
            "Train Epoch: 4 [5120/9000 (56%)]\tLoss: 4.640\n",
            "\n",
            "Test set: Average loss: 0.0184, Accuracy: 2002/10000 (20%)\n",
            "\n",
            "Train Epoch: 5 [0/9000 (0%)]\tLoss: 4.611\n",
            "Train Epoch: 5 [5120/9000 (56%)]\tLoss: 4.632\n",
            "\n",
            "Test set: Average loss: 0.0184, Accuracy: 2085/10000 (21%)\n",
            "\n",
            "Train Epoch: 6 [0/9000 (0%)]\tLoss: 4.622\n",
            "Train Epoch: 6 [5120/9000 (56%)]\tLoss: 4.624\n",
            "\n",
            "Test set: Average loss: 0.0184, Accuracy: 1972/10000 (20%)\n",
            "\n",
            "Train Epoch: 7 [0/9000 (0%)]\tLoss: 4.620\n",
            "Train Epoch: 7 [5120/9000 (56%)]\tLoss: 4.612\n",
            "\n",
            "Test set: Average loss: 0.0184, Accuracy: 2021/10000 (20%)\n",
            "\n",
            "Train Epoch: 8 [0/9000 (0%)]\tLoss: 4.624\n",
            "Train Epoch: 8 [5120/9000 (56%)]\tLoss: 4.612\n",
            "\n",
            "Test set: Average loss: 0.0184, Accuracy: 2169/10000 (22%)\n",
            "\n",
            "Train Epoch: 9 [0/9000 (0%)]\tLoss: 4.610\n",
            "Train Epoch: 9 [5120/9000 (56%)]\tLoss: 4.610\n",
            "\n",
            "Test set: Average loss: 0.0184, Accuracy: 1994/10000 (20%)\n",
            "\n",
            "Train Epoch: 10 [0/9000 (0%)]\tLoss: 4.636\n",
            "Train Epoch: 10 [5120/9000 (56%)]\tLoss: 4.634\n",
            "\n",
            "Test set: Average loss: 0.0184, Accuracy: 1961/10000 (20%)\n",
            "\n",
            "Train Epoch: 11 [0/9000 (0%)]\tLoss: 4.616\n",
            "Train Epoch: 11 [5120/9000 (56%)]\tLoss: 4.617\n",
            "\n",
            "Test set: Average loss: 0.0184, Accuracy: 2000/10000 (20%)\n",
            "\n",
            "Train Epoch: 12 [0/9000 (0%)]\tLoss: 4.629\n",
            "Train Epoch: 12 [5120/9000 (56%)]\tLoss: 4.600\n",
            "\n",
            "Test set: Average loss: 0.0184, Accuracy: 2101/10000 (21%)\n",
            "\n",
            "Train Epoch: 13 [0/9000 (0%)]\tLoss: 4.617\n",
            "Train Epoch: 13 [5120/9000 (56%)]\tLoss: 4.605\n",
            "\n",
            "Test set: Average loss: 0.0184, Accuracy: 1968/10000 (20%)\n",
            "\n",
            "Train Epoch: 14 [0/9000 (0%)]\tLoss: 4.618\n",
            "Train Epoch: 14 [5120/9000 (56%)]\tLoss: 4.613\n",
            "\n",
            "Test set: Average loss: 0.0184, Accuracy: 2000/10000 (20%)\n",
            "\n",
            "Train Epoch: 15 [0/9000 (0%)]\tLoss: 4.625\n",
            "Train Epoch: 15 [5120/9000 (56%)]\tLoss: 4.623\n",
            "\n",
            "Test set: Average loss: 0.0185, Accuracy: 2000/10000 (20%)\n",
            "\n",
            "CPU times: user 3min 35s, sys: 10.5 s, total: 3min 45s\n",
            "Wall time: 3min 45s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAjaQV6lv_zL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if importlib.util.find_spec('google.colab'):\n",
        "  from google.colab import files\n",
        "  files.download('mlruns')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tlOTbbSFAJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}